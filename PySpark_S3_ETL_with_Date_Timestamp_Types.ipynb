{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
        "from pyspark.sql.functions import col, lit, to_date, to_timestamp\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "import sys\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# This script demonstrates a 3-stage historical data loading pipeline:\n",
        "# 1. CSV (Source) -> Parquet (Intermediate)\n",
        "# 2. Parquet (Intermediate) -> Delta (Target)\n",
        "# 3. Efficient Incremental Load using MERGE.\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "# --- Configuration for AWS/Simulation ---\n",
        "# This path is where the final Delta Lake table resides (e.g., s3a://datalake-bucket/tables/products)\n",
        "DATALAKE_DELTA_PATH = \"/tmp/delta/product_sales_delta_table\"\n",
        "DATALAKE_S3_PATH = DATALAKE_DELTA_PATH\n",
        "\n",
        "# Source path (assumed CSV for the petabyte load)\n",
        "RAW_S3_PATH = \"s3a://your-raw-bucket/historical_sales_data_csv/\"\n",
        "# Intermediate path for high-performance Parquet storage\n",
        "INTERMEDIATE_PARQUET_PATH = \"/tmp/parquet/historical_parquet_intermediate/\"\n",
        "\n",
        "# TOGGLE FLAG: Set to False when running in a real AWS environment with data at RAW_S3_PATH\n",
        "USE_SIMULATED_HISTORY_DATA = True\n",
        "\n",
        "# 1. Create a SparkSession configured for Delta Lake\n",
        "print(\"Initializing Spark Session configured for Delta Lake...\")\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DeltaDataLoadingStrategies\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# 2. Define the schema for the final Delta table (Parquet stage will infer from this)\n",
        "# NOTE: When reading CSV, we often use StringType for all and then cast/transform.\n",
        "schema_target = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "    StructField(\"purchase_date\", DateType(), True),\n",
        "    StructField(\"last_updated\", TimestampType(), True),\n",
        "    StructField(\"year\", IntegerType(), True) # Partition Key\n",
        "])\n",
        "\n",
        "# --- UTILITY FUNCTIONS ---\n",
        "\n",
        "def prepare_data_for_load(df):\n",
        "    \"\"\"\n",
        "    Transforms the DataFrame by ensuring correct types and deriving the 'year' column.\n",
        "    Assumes incoming date columns might be strings if read from raw CSV/JSON.\n",
        "\n",
        "    NOTE: The date formats below must match the formats in your raw CSV data.\n",
        "    \"\"\"\n",
        "    # Assuming the raw input purchase_date is 'MM/dd/yyyy' and last_updated is 'yyyy-MM-dd HH:mm:ss'\n",
        "    df_transformed = df.withColumn(\"purchase_date\", to_date(col(\"purchase_date\"), \"MM/dd/yyyy\")) \\\n",
        "                       .withColumn(\"last_updated\", to_timestamp(col(\"last_updated\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "    # Derive the year partition key\n",
        "    return df_transformed.withColumn(\"year\", col(\"purchase_date\").substr(1, 4).cast(IntegerType()))\n",
        "\n",
        "def create_simulated_data(is_history=True):\n",
        "    \"\"\"Creates two different simulated datasets for the two loading types.\"\"\"\n",
        "    # Note: Using String format for dates/timestamps here to simulate CSV raw read\n",
        "    current_ts_str = datetime.now().strftime(\"2024-07-20 10:30:00\") # Fixed for simulation simplicity\n",
        "\n",
        "    if is_history:\n",
        "        # Simulate a large, multi-year dataset (History)\n",
        "        print(\"Simulating large historical dataset across multiple years (raw CSV format)...\")\n",
        "        history_data = [\n",
        "            (101, \"Server Rack\", 5000.00, \"01/15/2022\", current_ts_str),\n",
        "            (102, \"Fiber Cable\", 150.00, \"12/01/2022\", current_ts_str),\n",
        "            (201, \"Laptop Pro\", 2200.00, \"03/22/2023\", current_ts_str),\n",
        "            (202, \"Monitor 4K\", 650.00, \"10/10/2023\", current_ts_str),\n",
        "            (301, \"Keyboard Mech\", 120.00, \"01/01/2024\", current_ts_str), # Target for incremental update\n",
        "            (302, \"Mouse Erg\", 75.00, \"06/05/2024\", current_ts_str)\n",
        "        ]\n",
        "        # Use StringType for date/timestamp fields to simulate raw CSV read\n",
        "        raw_schema = StructType([\n",
        "            StructField(\"id\", IntegerType(), True),\n",
        "            StructField(\"product_name\", StringType(), True),\n",
        "            StructField(\"price\", DoubleType(), True),\n",
        "            StructField(\"purchase_date\", StringType(), True),\n",
        "            StructField(\"last_updated\", StringType(), True)\n",
        "        ])\n",
        "        return spark.createDataFrame(history_data, schema=raw_schema)\n",
        "\n",
        "    else:\n",
        "        # Simulate a small, incremental batch (New data for 2024 and 2025)\n",
        "        print(\"Simulating small incremental dataset (new and updated records)...\")\n",
        "        next_year_str = str(datetime.now().year + 1)\n",
        "        current_ts_incr_str = datetime.now().strftime(\"2024-07-20 10:30:01\")\n",
        "\n",
        "        incremental_data = [\n",
        "            # 1. Update existing record (ID 301 from 2024)\n",
        "            (301, \"Keyboard Mech\", 99.00, \"01/01/2024\", current_ts_incr_str),\n",
        "            # 2. Insert new record for current year (2024)\n",
        "            (303, \"Webcam HD\", 50.00, \"10/05/2024\", current_ts_incr_str),\n",
        "            # 3. Insert new record for a future partition (2025)\n",
        "            (401, \"VR Headset\", 1500.00, \"01/01/\" + next_year_str, current_ts_incr_str)\n",
        "        ]\n",
        "        # Use StringType for date/timestamp fields to simulate raw CSV read\n",
        "        raw_schema = StructType([\n",
        "            StructField(\"id\", IntegerType(), True),\n",
        "            StructField(\"product_name\", StringType(), True),\n",
        "            StructField(\"price\", DoubleType(), True),\n",
        "            StructField(\"purchase_date\", StringType(), True),\n",
        "            StructField(\"last_updated\", StringType(), True)\n",
        "        ])\n",
        "        return spark.createDataFrame(incremental_data, schema=raw_schema)\n",
        "\n",
        "\n",
        "def check_path_existence(spark_session: SparkSession, path: str, format_type: str = \"delta\") -> bool:\n",
        "    # Simplified path check logic for demonstration\n",
        "    if format_type == \"delta\" and DeltaTable.isDeltaTable(spark_session, path):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# --- LOAD STRATEGIES ---\n",
        "\n",
        "def ingest_historical_data_aws_glue(spark):\n",
        "    \"\"\"\n",
        "    1. HISTORY LOAD (Petabytes): Implements the two-stage conversion: CSV -> Parquet -> Delta.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 1: AWS GLUE/SPARK HISTORICAL INGESTION (3-STAGE CONVERSION)\")\n",
        "    print(\"Stage 1: CSV -> Parquet (Intermediate)\")\n",
        "    print(\"Stage 2: Parquet -> Delta (Target, Partitioned by 'year')\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # --- STAGE 1A: READ RAW CSV DATA ---\n",
        "    if USE_SIMULATED_HISTORY_DATA:\n",
        "        df_history_raw = create_simulated_data(is_history=True)\n",
        "        print(f\"Reading {df_history_raw.count()} simulated records (Simulation Mode).\")\n",
        "    else:\n",
        "        print(f\"Attempting to read massive CSV data from S3 Source: {RAW_S3_PATH}...\")\n",
        "        try:\n",
        "            # Assumes CSV format with header for the petabyte source\n",
        "            df_history_raw = spark.read.csv(\n",
        "                RAW_S3_PATH,\n",
        "                header=True,\n",
        "                inferSchema=False # Use StringType for max compatibility with raw CSV\n",
        "            )\n",
        "            print(f\"Successfully read {df_history_raw.count()} records from S3.\")\n",
        "        except AnalysisException as e:\n",
        "            print(f\"ERROR: Could not read data from {RAW_S3_PATH}. Check S3 path and IAM permissions.\")\n",
        "            raise e\n",
        "\n",
        "    # --- STAGE 1B: TRANSFORM AND WRITE TO INTERMEDIATE PARQUET ---\n",
        "    # Apply type casting and derive the year column\n",
        "    df_history_transformed = prepare_data_for_load(df_history_raw)\n",
        "\n",
        "    print(\"\\n--- Writing to Intermediate Parquet ---\")\n",
        "    df_history_transformed.write \\\n",
        "        .format(\"parquet\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .partitionBy(\"year\") \\\n",
        "        .save(INTERMEDIATE_PARQUET_PATH)\n",
        "\n",
        "    print(f\"Intermediate Parquet Load Complete at {INTERMEDIATE_PARQUET_PATH}\")\n",
        "    print(f\"Parquet format offers high read performance for the next stage.\")\n",
        "\n",
        "    # --- STAGE 2: READ INTERMEDIATE PARQUET AND WRITE TO DELTA ---\n",
        "    print(\"\\n--- Reading Parquet and Writing to Final Delta ---\")\n",
        "    df_intermediate = spark.read.format(\"parquet\").load(INTERMEDIATE_PARQUET_PATH)\n",
        "\n",
        "    df_intermediate.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .partitionBy(\"year\") \\\n",
        "        .save(DATALAKE_DELTA_PATH)\n",
        "\n",
        "    print(f\"Final Delta Load Complete at {DATALAKE_DELTA_PATH}\")\n",
        "\n",
        "    print(\"Initial Delta Table Content:\")\n",
        "    DeltaTable.forPath(spark, DATALAKE_DELTA_PATH).history().select(\"version\", \"operation\").show(1, truncate=False)\n",
        "    spark.read.format(\"delta\").load(DATALAKE_DELTA_PATH).orderBy(\"id\").show()\n",
        "\n",
        "\n",
        "def load_incremental_data(spark):\n",
        "    \"\"\"\n",
        "    2. INCREMENTAL LOAD: Efficiently updates existing data and inserts new records\n",
        "       using Delta MERGE, leveraging the 'year' partition key.\n",
        "    \"\"\"\n",
        "    df_source_increment_raw = create_simulated_data(is_history=False)\n",
        "    df_source_increment = prepare_data_for_load(df_source_increment_raw)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 2: INCREMENTAL LOAD (Partition Key 'year')\")\n",
        "    print(\"Applying incremental changes via MERGE (Update, Insert).\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if not check_path_existence(spark, DATALAKE_DELTA_PATH, format_type=\"delta\"):\n",
        "        print(f\"FATAL ERROR: Delta Table not initialized at {DATALAKE_DELTA_PATH}. Aborting incremental load.\")\n",
        "        return\n",
        "\n",
        "    delta_table = DeltaTable.forPath(spark, DATALAKE_DELTA_PATH)\n",
        "\n",
        "    # Delta MERGE based on ID and YEAR (Crucial for Partition Alignment)\n",
        "    delta_table.alias(\"target\") \\\n",
        "        .merge(\n",
        "            source = df_source_increment.alias(\"source\"),\n",
        "            # Match on both primary key (id) and partition key (year) for precision\n",
        "            condition = \"target.id = source.id AND target.year = source.year\"\n",
        "        ) \\\n",
        "        .whenMatchedUpdate( # Update logic (e.g., price changed)\n",
        "            condition = \"target.price != source.price\",\n",
        "            set = {\n",
        "                \"price\": col(\"source.price\"),\n",
        "                \"last_updated\": lit(datetime.now())\n",
        "            }\n",
        "        ) \\\n",
        "        .whenNotMatchedInsertAll() \\\n",
        "        .execute()\n",
        "\n",
        "    print(\"Incremental Load Complete.\")\n",
        "\n",
        "    print(\"Final Delta Table Content after MERGE:\")\n",
        "    spark.read.format(\"delta\").load(DATALAKE_DELTA_PATH).orderBy(\"id\").show()\n",
        "\n",
        "try:\n",
        "    # --- EXECUTION FLOW ---\n",
        "\n",
        "    # 1. Perform the initial History Load (CSV -> Parquet -> Delta)\n",
        "    ingest_historical_data_aws_glue(spark)\n",
        "\n",
        "    # 2. Perform the subsequent Incremental Load\n",
        "    load_incremental_data(spark)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ERROR ---\")\n",
        "    print(f\"An error occurred during data loading: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "finally:\n",
        "    # Stop the SparkSession\n",
        "    print(\"-\" * 60)\n",
        "    spark.stop()\n",
        "    print(\"Spark Session stopped.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "S8O-JxsuAUo-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}